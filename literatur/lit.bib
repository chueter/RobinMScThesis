
@inproceedings{adebayoSanityChecksSaliency2018a,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  shorttitle = {Sanity {{Checks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} 2018},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian J. and Hardt, Moritz and Kim, Been},
  date = {2018},
  volume = {31},
  pages = {9525--9536},
  location = {{Montr\'eal, Canada}},
  url = {http://papers.nips.cc/paper/8160-sanity-checks-for-saliency-maps},
  abstract = {Zusammengefasst in Guided Grad-CAM is Broken! Sanity Checks for Saliency Maps},
  eventtitle = {{{NeurIPS}} 2018},
  file = {/Users/robin/Documents/Literatur/Adebayo et al_2018_Sanity Checks for Saliency Maps2.pdf},
  keywords = {Zusammengefasst},
  langid = {english},
  series = {Advances in {{Neural Information Processing Systems}}}
}

@inproceedings{agrawalAnalyzingBehaviorVisual2016,
  title = {Analyzing the {{Behavior}} of {{Visual Question Answering Models}}},
  shorttitle = {Analyzing {{Behavior}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  date = {2016},
  pages = {1955--1960},
  publisher = {{Association for Computational Linguistics}},
  location = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1203},
  url = {http://aclweb.org/anthology/D16-1203},
  urldate = {2020-07-02},
  eventtitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  file = {/Users/robin/Documents/Literatur/Agrawal et al_2016_Analyzing the Behavior of Visual Question Answering Models.pdf},
  langid = {english}
}

@online{AICompute2018,
  title = {{{AI}} and {{Compute}}},
  date = {2018-05-16T15:59:43.000Z},
  journaltitle = {OpenAI},
  url = {https://openai.com/blog/ai-and-compute/},
  urldate = {2020-07-27},
  abstract = {We're releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore's Law had a 2-year doubling period).},
  file = {/Users/robin/Documents/Literatur/2018_AI and Compute.pdf;/Users/robin/Zotero/storage/HAGH536W/ai-and-compute.html},
  langid = {english}
}

@online{AIEfficiency2020,
  title = {{{AI}} and {{Efficiency}}},
  date = {2020-05-05T15:58:44.000Z},
  journaltitle = {OpenAI},
  url = {https://openai.com/blog/ai-and-efficiency/},
  urldate = {2020-07-27},
  abstract = {We're releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a},
  file = {/Users/robin/Documents/Literatur/2020_AI and Efficiency.pdf;/Users/robin/Zotero/storage/3M7RCQWN/ai-and-efficiency.html},
  langid = {english}
}

@online{alexConceptsPillowPIL2020,
  title = {Concepts \textemdash{} {{Pillow}} ({{PIL Fork}}) 7.2.0 Documentation},
  shorttitle = {Filtervergleich},
  author = {Alex, Clark},
  date = {2020-07-21},
  journaltitle = {pillow.readthedocs.io},
  url = {https://pillow.readthedocs.io/en/stable/handbook/concepts.html#filters-comparison-table},
  urldate = {2020-07-21},
  abstract = {PIL Concepts, Filter comparison},
  file = {/Users/robin/Zotero/storage/WRMWBNGT/Alex - 2020 - Concepts — Pillow (PIL Fork) 7.2.0 documentation.pdf;/Users/robin/Zotero/storage/JFBPINY4/concepts.html},
  langid = {english},
  type = {Documentation}
}

@article{arcos-garciaDeepNeuralNetwork2018,
  title = {Deep Neural Network for Traffic Sign Recognition Systems: {{An}} Analysis of Spatial Transformers and Stochastic Optimisation Methods},
  shorttitle = {{{CNN}} Mit 3 {{Spatial Transformers}}},
  author = {Arcos-Garc\'ia, \'Alvaro and \'Alvarez-Garc\'ia, Juan A. and Soria-Morillo, Luis M.},
  date = {2018},
  journaltitle = {Neural Networks},
  volume = {99},
  pages = {158--165},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.01.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608018300054},
  abstract = {This paper presents a Deep Learning approach for traffic sign recognition systems. Several classification experiments are conducted over publicly available traffic sign datasets from Germany and Belgium using a Deep Neural Network which comprises Convolutional layers and Spatial Transformer Networks. Such trials are built to measure the impact of diverse factors with the end goal of designing a Convolutional Neural Network that can improve the state-of-the-art of traffic sign classification task. First, different adaptive and non-adaptive stochastic gradient descent optimisation algorithms such as SGD, SGD-Nesterov, RMSprop and Adam are evaluated. Subsequently, multiple combinations of Spatial Transformer Networks placed at distinct positions within the main neural network are analysed. The recognition rate of the proposed Convolutional Neural Network reports an accuracy of 99.71\% in the German Traffic Sign Recognition Benchmark, outperforming previous state-of-the-art methods and also being more efficient in terms of memory requirements.},
  file = {/Users/robin/Documents/Literatur/Arcos-García et al_2018_Deep neural network for traffic sign recognition systems.pdf},
  keywords = {Convolutional neural network,Deep learning,Spatial transformer network,Traffic sign}
}

@online{bastBASt2017Fachthemen2020,
  title = {BASt 2017 - Fachthemen - Verkehrszeichen und Symbole},
  shorttitle = {\"Ubersicht Verkehrszeichen},
  author = {BAst},
  date = {2020-06-21},
  journaltitle = {Verkehrszeichen und Symbole},
  url = {https://www.bast.de/BASt_2017/DE/Verkehrstechnik/Fachthemen/v1-verkehrszeichen/vz-download.html},
  urldate = {2020-06-21},
  abstract = {Overview of streetsigns in germany.},
  file = {/Users/robin/Zotero/storage/EGDXPP7E/vz-download.html},
  langid = {Deutsch}
}

@inproceedings{bazzaniSelftaughtObjectLocalization2016,
  title = {Self-Taught Object Localization with Deep Networks},
  shorttitle = {Self-Taught Object Localization},
  booktitle = {2016 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Bazzani, Loris and Bergamo, Alessandra and Anguelov, Dragomir and Torresani, Lorenzo},
  date = {2016-03},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Lake Placid, NY, USA}},
  doi = {10.1109/WACV.2016.7477688},
  url = {http://ieeexplore.ieee.org/document/7477688/},
  urldate = {2020-07-02},
  eventtitle = {2016 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  file = {/Users/robin/Documents/Literatur/Bazzani et al_2016_Self-taught object localization with deep networks.pdf},
  isbn = {978-1-5090-0641-0},
  langid = {english}
}

@online{beckerKopfKopf2020,
  title = {Kopf an Kopf},
  shorttitle = {Entwicklung autonomes Fahren},
  author = {Becker, Joachim},
  date = {2020-06-20},
  journaltitle = {S\"uddeutsche Zeitung Digitale Medien GmbH},
  url = {http://sz.de/1.4940396},
  urldate = {2020-06-21},
  abstract = {Vorsprung durch Technik - das war einmal. Jetzt geht es um Vorsprung durch Elektronik und digitale Dienste: Bei regelm\"a\ss igen Software-Updates \"uber die Luftschnittstelle und Fahrassistenten kann BMW zu Tesla aufschlie\ss en.},
  langid = {Deusch},
  type = {Zeitungsartikel}
}

@inproceedings{belaroussiRoadSignDetection2010,
  title = {Road {{Sign Detection}} in {{Images}}: {{A Case Study}}},
  shorttitle = {Stereopolis {{Database}}},
  booktitle = {2010 20th {{International Conference}} on {{Pattern Recognition}}},
  author = {Belaroussi, Rachid and Foucher, Philippe and Tarel, Jean-Philippe and Soheilian, Bahman and Charbonnier, Pierre and Paparoditis, Nicolas},
  date = {2010-08},
  pages = {484--488},
  publisher = {{IEEE}},
  location = {{Istanbul, Turkey}},
  doi = {10.1109/ICPR.2010.1125},
  url = {http://ieeexplore.ieee.org/document/5597423/},
  urldate = {2020-10-19},
  eventtitle = {2010 20th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  file = {/Users/robin/Documents/Literatur/Belaroussi et al_2010_Road Sign Detection in Images.pdf},
  isbn = {978-1-4244-7542-1}
}

@article{budaSystematicStudyClass2018,
  title = {A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks},
  author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
  date = {2018-10},
  journaltitle = {Neural Networks},
  volume = {106},
  pages = {249--259},
  issn = {08936080},
  doi = {10.1016/j.neunet.2018.07.011},
  url = {http://arxiv.org/abs/1710.05381},
  urldate = {2020-08-30},
  abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
  archivePrefix = {arXiv},
  eprint = {1710.05381},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Buda et al_2018_A systematic study of the class imbalance problem in convolutional neural2.pdf;/Users/robin/Zotero/storage/32VIHU9K/1710.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{ciresanMulticolumnDeepNeural2012,
  title = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  author = {Cire\c{s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J\"urgen},
  date = {2012-08},
  journaltitle = {Neural Networks},
  volume = {32},
  pages = {333--338},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.02.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608012000524},
  urldate = {2020-06-04},
  file = {/Users/robin/Documents/Literatur/Cireşan et al_2012_Multi-column deep neural network for traffic sign classification.pdf},
  langid = {english}
}

@article{ciresanMulticolumnDeepNeural2012a,
  title = {Multi-Column Deep Neural Network for Traffic Sign Classification},
  shorttitle = {Zusammenschluss Mehrerer {{CNNs}}},
  author = {Cire\c{s}an, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, J\"urgen},
  date = {2012-08},
  journaltitle = {Neural Networks},
  volume = {32},
  pages = {333--338},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.02.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608012000524},
  urldate = {2020-10-19},
  file = {/Users/robin/Documents/Literatur/Cireşan et al_2012_Multi-column deep neural network for traffic sign classification2.pdf},
  langid = {english}
}

@online{dicksonDangersTrustingBlackbox2020,
  title = {The Dangers of Trusting Black-Box Machine Learning},
  author = {Dickson, Ben},
  date = {2020-07-27T13:00:40+00:00},
  journaltitle = {TechTalks},
  url = {https://bdtechtalks.com/2020/07/27/black-box-ai-models/},
  urldate = {2020-07-27},
  abstract = {Diskussion zu Paper: "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"},
  file = {/Users/robin/Documents/Literatur/Dickson_2020_The dangers of trusting black-box machine learning.pdf;/Users/robin/Zotero/storage/Y4BS837N/black-box-ai-models.html},
  langid = {american}
}

@article{dosovitskiyInvertingConvolutionalNetworks2015,
  title = {Inverting {{Convolutional Networks}} with {{Convolutional Networks}}},
  shorttitle = {Inverting {{Representation}}},
  author = {Dosovitskiy, Alexey and Brox, Thomas},
  date = {2015},
  journaltitle = {CoRR},
  volume = {abs/1506.02753},
  url = {http://arxiv.org/abs/1506.02753},
  annotation = {\_eprint: 1506.02753},
  file = {/Users/robin/Documents/Literatur/Dosovitskiy_Brox_2015_Inverting Convolutional Networks with Convolutional Networks.pdf}
}

@online{draelosCNNHeatMaps2019,
  title = {{{CNN Heat Maps}}: {{Saliency}}/{{Backpropagation}}},
  shorttitle = {{{CNN Heat Maps}}},
  author = {Draelos, Rachel},
  date = {2019-06-21T16:11:42+00:00},
  journaltitle = {Glass Box},
  url = {https://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/},
  urldate = {2020-07-05},
  abstract = {Ausf\"uhrliche Beschreibung wie "vanilla" Backpropagation f\"ur Saliency Maps funktioniert. Verwendet wird die "normale" ReLU Funktion.},
  file = {/Users/robin/Documents/Literatur/Draelos_2019_CNN Heat Maps.pdf;/Users/robin/Zotero/storage/E6HJCAXS/cnn-heat-maps-saliency-backpropagation.html},
  keywords = {Gelesen,Markiert,Zusammengefasst},
  langid = {english}
}

@online{draelosCNNHeatMaps2019a,
  title = {{{CNN Heat Maps}}: {{Gradients}} vs. {{DeconvNets}} vs. {{Guided Backpropagation}}},
  shorttitle = {{{CNN Heat Maps}}},
  author = {Draelos, Rachel},
  date = {2019-10-06T01:34:36+00:00},
  journaltitle = {Glass Box},
  url = {https://glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/},
  urldate = {2020-07-05},
  abstract = {- Vergleicht die drei Methoden - Erl\"autert Grundlagen der drei Methoden Alle drei Methoden verfolgen den selben Ansatz, unterschiedlich ist nur die Backpropagation im Rahmen der ReLU Funktion.},
  file = {/Users/robin/Documents/Literatur/Draelos_2019_CNN Heat Maps (VS).pdf;/Users/robin/Zotero/storage/DIF7GDQS/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation.html},
  keywords = {Zusammengefasst},
  langid = {english}
}

@online{draelosGuidedGradCAMBroken2019,
  title = {Guided {{Grad}}-{{CAM}} Is {{Broken}}! {{Sanity Checks}} for {{Saliency Maps}}},
  author = {Draelos, Rachel},
  date = {2019-10-12T22:01:50+00:00},
  journaltitle = {Glass Box},
  url = {https://glassboxmedicine.com/2019/10/12/guided-grad-cam-is-broken-sanity-checks-for-saliency-maps/},
  urldate = {2020-07-05},
  abstract = {Wrapper um das Paper Sanity Checks for Saliency Maps von Adebayo et al. Beschreibt 2 Sanity Checks um zu pr\"ufen, ob Methoden f\"ur SM sinnvolle SM erstellen. - Model Parameter Randomization -{$>$} Gewichte werden randomisiert - Data Randomization -{$>$} Label werden randomosiert Ergebnis ist, dass Guided GradCam und Guided Backpropagation durchfallen! Zu verwenden sind Gradient und GradCAM!},
  file = {/Users/robin/Documents/Literatur/Draelos_2019_Guided Grad-CAM is Broken.pdf},
  keywords = {Zusammengefasst},
  langid = {english}
}

@online{dumoulinGuideConvolutionArithmetic2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  shorttitle = {Output {{Convolutional Layer}}},
  author = {Dumoulin, Vincent and Visin, Francesco},
  date = {2018-01-11},
  pages = {15},
  url = {http://arxiv.org/abs/1603.07285},
  urldate = {2020-10-17},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archivePrefix = {arXiv},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Dumoulin_Visin_2018_A guide to convolution arithmetic for deep learning.pdf;/Users/robin/Zotero/storage/Q8R7PZPZ/1603.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{ertlerMapillaryTrafficSign2020,
  title = {The {{Mapillary Traffic Sign Dataset}} for {{Detection}} and {{Classification}} on a {{Global Scale}}},
  author = {Ertler, Christian and Mislej, Jerneja and Ollmann, Tobias and Porzi, Lorenzo and Neuhold, Gerhard and Kuang, Yubin},
  date = {2020-05-07},
  url = {http://arxiv.org/abs/1909.04422},
  urldate = {2020-10-19},
  abstract = {Traffic signs are essential map features globally in the era of autonomous driving and smart cities. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a traffic sign benchmark dataset of 100K street-level images around the world that encapsulates diverse scenes, wide coverage of geographical locations, and varying weather and lighting conditions and covers more than 300 manually annotated traffic sign classes. The dataset includes 52K images that are fully annotated and 48K images that are partially annotated. This is the largest and the most diverse traffic sign dataset consisting of images from all over world with fine-grained annotations of traffic sign classes. We have run extensive experiments to establish strong baselines for both the detection and the classification tasks. In addition, we have verified that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research: https://www.mapillary.com/dataset/trafficsign.},
  archivePrefix = {arXiv},
  eprint = {1909.04422},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Ertler et al_2020_The Mapillary Traffic Sign Dataset for Detection and Classification on a Global2.pdf;/Users/robin/Zotero/storage/JSW9TT7R/1909.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@software{filterJfilterSplitfolders2020,
  title = {Jfilter/Split-Folders},
  shorttitle = {Split-Folders},
  author = {Filter, Johannes},
  date = {2020-07-23T10:05:11Z},
  origdate = {2018-10-04T12:33:35Z},
  url = {https://github.com/jfilter/split-folders},
  urldate = {2020-07-24},
  abstract = {Split folders with files (i.e. images) into training, validation and test (dataset) folders},
  file = {/Users/robin/Zotero/storage/H5KBWGWF/Filter - 2020 - jfiltersplit-folders.pdf},
  keywords = {dataset,deep-learning,machine-learning,oversampling,splitting,test,training,validation}
}

@incollection{fukushima1982neocognitron,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition},
  shorttitle = {Neocognitron},
  booktitle = {Competition and Cooperation in Neural Nets},
  author = {Fukushima, Kunihiko and Miyake, Sei},
  date = {1982},
  pages = {267--285},
  publisher = {{Springer}},
  file = {/Users/robin/Documents/Literatur/Fukushima_Miyake_1982_Neocognitron.pdf}
}

@article{gecerColorblobbasedCOSFIREFilters2017,
  title = {Color-Blob-Based {{COSFIRE}} Filters for Object Recognition},
  shorttitle = {{{COSFIRE}}-{{Filter}}},
  author = {Gecer, Baris and Azzopardi, George and Petkov, Nicolai},
  date = {2017-01},
  journaltitle = {Image and Vision Computing},
  volume = {57},
  pages = {165--174},
  issn = {02628856},
  doi = {10.1016/j.imavis.2016.10.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0262885616301895},
  urldate = {2020-10-19},
  file = {/Users/robin/Documents/Literatur/Gecer et al_2017_Color-blob-based COSFIRE filters for object recognition.pdf},
  langid = {english}
}

@book{geronHandsOnMachineLearning2019,
  title = {Hands-{{On Machine Learning}} with {{Scikit}}-{{Learn}}, {{Keras}}, and {{TensorFlow}}: {{Concepts}}, {{Tools}}, and {{Techniques}} to {{Build Intelligent Systems}}.},
  shorttitle = {Hands-{{On Machine Learning}} with {{Scikit}}-{{Learn}}, {{Keras}}, and {{TensorFlow}}},
  author = {Geron, Aurelien},
  date = {2019},
  publisher = {{O'Reilly Media, Incorporated}},
  annotation = {OCLC: 1107130994},
  isbn = {978-1-4920-3264-9},
  langid = {english}
}

@inproceedings{girshickRichFeatureHierarchies2014,
  title = {Rich {{Feature Hierarchies}} for {{Accurate Object Detection}} and {{Semantic Segmentation}}},
  shorttitle = {Object {{Detection}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014-06},
  pages = {580--587},
  publisher = {{IEEE}},
  location = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.81},
  url = {http://ieeexplore.ieee.org/document/6909475/},
  urldate = {2020-07-02},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Girshick et al_2014_Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.pdf},
  isbn = {978-1-4799-5118-5}
}

@online{guBadNetsIdentifyingVulnerabilities2019,
  title = {{{BadNets}}: {{Identifying Vulnerabilities}} in the {{Machine Learning Model Supply Chain}}},
  shorttitle = {{{BadNets}}},
  author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
  date = {2019-03-11},
  pages = {6--7},
  url = {http://arxiv.org/abs/1708.06733},
  urldate = {2020-06-14},
  abstract = {Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a \textbackslash emph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}\textbackslash\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.},
  archivePrefix = {arXiv},
  eprint = {1708.06733},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Gu et al_2019_BadNets.pdf;/Users/robin/Zotero/storage/KXEX8JGI/1708.html},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016-06},
  pages = {770--778},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  url = {http://ieeexplore.ieee.org/document/7780459/},
  urldate = {2020-07-02},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/He et al_2016_Deep Residual Learning for Image Recognition.pdf},
  isbn = {978-1-4673-8851-1}
}

@incollection{hoiemDiagnosingErrorObject2012,
  title = {Diagnosing {{Error}} in {{Object Detectors}}},
  shorttitle = {Diagnosing {{Error}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2012},
  author = {Hoiem, Derek and Chodpathumwan, Yodsawalai and Dai, Qieyun},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  date = {2012},
  volume = {7574},
  pages = {340--353},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33712-3_25},
  url = {http://link.springer.com/10.1007/978-3-642-33712-3_25},
  urldate = {2020-07-02},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  file = {/Users/robin/Documents/Literatur/Hoiem et al_2012_Diagnosing Error in Object Detectors.pdf},
  isbn = {978-3-642-33711-6 978-3-642-33712-3},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{hookerBenchmarkInterpretabilityMethods2019a,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  shorttitle = {{{ROAR}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alch\'e-Buc, F. and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {9737--9748},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/9167-a-benchmark-for-interpretability-methods-in-deep-neural-networks.pdf},
  file = {/Users/robin/Documents/Literatur/Hooker et al_2019_A Benchmark for Interpretability Methods in Deep Neural Networks.pdf}
}

@article{hubelReceptiveFieldsBinocular1962,
  title = {Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex},
  shorttitle = {Binocular {{Interaction}}},
  author = {Hubel, David H and Wiesel, Torsten N},
  date = {1962},
  journaltitle = {The Journal of physiology},
  volume = {160},
  pages = {106--154},
  publisher = {{Wiley-Blackwell}},
  file = {/Users/robin/Documents/Literatur/Hubel_Wiesel_1962_Receptive fields, binocular interaction and functional architecture in the.pdf},
  number = {1}
}

@article{hubelReceptiveFieldsFunctional1968,
  title = {Receptive Fields and Functional Architecture of Monkey Striate Cortex},
  author = {Hubel, D. H. and Wiesel, T. N.},
  date = {1968-03-01},
  journaltitle = {The Journal of Physiology},
  volume = {195},
  pages = {215--243},
  issn = {00223751},
  doi = {10.1113/jphysiol.1968.sp008455},
  url = {http://doi.wiley.com/10.1113/jphysiol.1968.sp008455},
  urldate = {2020-10-17},
  file = {/Users/robin/Documents/Literatur/Hubel_Wiesel_1968_Receptive fields and functional architecture of monkey striate cortex.pdf},
  langid = {english},
  number = {1}
}

@article{hubelReceptiveFieldsSingle1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  shorttitle = {Single {{Neurons}}},
  author = {Hubel, D. H. and Wiesel, T. N.},
  date = {1959-10-01},
  journaltitle = {The Journal of Physiology},
  volume = {148},
  pages = {574--591},
  issn = {00223751},
  doi = {10.1113/jphysiol.1959.sp006308},
  url = {http://doi.wiley.com/10.1113/jphysiol.1959.sp006308},
  urldate = {2020-10-17},
  file = {/Users/robin/Documents/Literatur/Hubel_Wiesel_1959_Receptive fields of single neurones in the cat's striate cortex.pdf},
  langid = {english},
  number = {3}
}

@book{jacksonIntroductionExpertSystems1999,
  title = {Introduction to {{Expert Systems}}, 3rd {{Edition}}},
  author = {Jackson, Peter},
  date = {1999},
  edition = {3.},
  publisher = {{Addison-Wesley}},
  location = {{Boston, MA, USA}},
  url = {http://members.aol.com/JacksonPE/music1/introduc.htm},
  isbn = {978-0-201-87686-4},
  langid = {english},
  pagetotal = {560},
  series = {International {{Computer Science Series}}}
}

@inproceedings{jaderbergSpatialTransformerNetworks2015,
  title = {Spatial {{Transformer Networks}}},
  shorttitle = {Spatial {{Transformer Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28: {{Annual Conference}} on {{Neural Information Processing Systems}} 2015},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  date = {2015},
  pages = {2017--2025},
  location = {{Montreal, Quebec, Canada}},
  url = {http://papers.nips.cc/paper/5854-spatial-transformer-networks},
  eventtitle = {{{NeurIPS}} 2015},
  series = {Advances in {{Neural Information Processing Systems}}}
}

@inproceedings{johnsBecomingExpertInteractive2015,
  title = {Becoming the Expert - Interactive Multi-Class Machine Teaching},
  shorttitle = {Learning from {{Models}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Johns, Edward and Aodha, Oisin Mac and Brostow, Gabriel J.},
  date = {2015-06},
  pages = {2616--2624},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298877},
  url = {http://ieeexplore.ieee.org/document/7298877/},
  urldate = {2020-07-02},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Johns et al_2015_Becoming the expert - interactive multi-class machine teaching.pdf},
  isbn = {978-1-4673-6964-0}
}

@incollection{kindermansReliabilitySaliencyMethods2019,
  title = {The ({{Un}})Reliability of {{Saliency Methods}}},
  booktitle = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch\"utt, Kristof T. and D\"ahne, Sven and Erhan, Dumitru and Kim, Been},
  date = {2019},
  volume = {11700},
  pages = {267--280},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-28954-6_14},
  url = {http://link.springer.com/10.1007/978-3-030-28954-6_14},
  urldate = {2020-07-05},
  file = {/Users/robin/Documents/Literatur/Kindermans et al_2019_The (Un)reliability of Saliency Methods.pdf},
  isbn = {978-3-030-28953-9 978-3-030-28954-6},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  shorttitle = {{{ImageNet CNN}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  date = {2017-05-24},
  journaltitle = {Commun. ACM},
  volume = {60},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  url = {https://dl.acm.org/doi/10.1145/3065386},
  urldate = {2020-07-02},
  file = {/Users/robin/Documents/Literatur/Krizhevsky et al_2017_ImageNet classification with deep convolutional neural networks.pdf},
  langid = {english},
  number = {6}
}

@book{lanhamRevisingProse2007,
  title = {Revising Prose},
  author = {Lanham, Richard A.},
  date = {2007},
  edition = {5th ed},
  publisher = {{Pearson Longman}},
  location = {{New York}},
  isbn = {978-0-321-44169-0},
  keywords = {Editing,English language,Rhetoric,Style},
  pagetotal = {166}
}

@incollection{larssonUsingFourierDescriptors2011,
  title = {Using {{Fourier Descriptors}} and {{Spatial Models}} for {{Traffic Sign Recognition}}},
  shorttitle = {Datensatz {{Sommer Schweden}}},
  booktitle = {Image {{Analysis}}},
  author = {Larsson, Fredrik and Felsberg, Michael},
  editor = {Heyden, Anders and Kahl, Fredrik},
  date = {2011},
  volume = {6688},
  pages = {238--249},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21227-7_23},
  url = {http://link.springer.com/10.1007/978-3-642-21227-7_23},
  urldate = {2020-10-19},
  file = {/Users/robin/Documents/Literatur/Larsson_Felsberg_2011_Using Fourier Descriptors and Spatial Models for Traffic Sign Recognition.pdf},
  isbn = {978-3-642-21226-0 978-3-642-21227-7},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{lecun1990handwritten,
  title = {Handwritten Digit Recognition with a Back-Propagation Network},
  shorttitle = {Handwritten Digit Recognition},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {LeCun, Yann and Boser, Bernhard E and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne E and Jackel, Lawrence D},
  date = {1990},
  pages = {396--404},
  file = {/Users/robin/Documents/Literatur/LeCun et al_1990_Handwritten digit recognition with a back-propagation network.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  shorttitle = {Backpropagation {{CNN}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  date = {1989-12},
  journaltitle = {Neural Computation},
  volume = {1},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
  urldate = {2020-07-04},
  file = {/Users/robin/Documents/Literatur/LeCun et al_1989_Backpropagation Applied to Handwritten Zip Code Recognition.pdf},
  langid = {english},
  number = {4}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  shorttitle = {Gradient-Based Learning},
  author = {LeCun, Yann and Bottou, L\'eon and Bengio, Yoshua and Haffner, Patrick},
  date = {1998},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  pages = {2278--2324},
  publisher = {{Ieee}},
  file = {/Users/robin/Documents/Literatur/LeCun et al_1998_Gradient-based learning applied to document recognition.pdf},
  number = {11}
}

@inproceedings{linNetworkNetwork2014,
  title = {Network {{In Network}}},
  shorttitle = {Network in {{Network}}},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Conference Track Proceedings}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2014},
  pages = {14--16},
  location = {{Banff, AB, Canada}},
  url = {http://arxiv.org/abs/1312.4400},
  urldate = {2020-07-02},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/Users/robin/Documents/Literatur/Lin et al_2014_Network In Network.pdf},
  langid = {english}
}

@article{liptonMythosModelInterpretability2018,
  title = {The Mythos of Model Interpretability},
  shorttitle = {Model Interpretability},
  author = {Lipton, Zachary C.},
  date = {2018-09-26},
  journaltitle = {Commun. ACM},
  volume = {61},
  pages = {36--43},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3233231},
  url = {https://dl.acm.org/doi/10.1145/3233231},
  urldate = {2020-07-02},
  file = {/Users/robin/Documents/Literatur/Lipton_2018_The mythos of model interpretability.pdf},
  keywords = {Zusammengefasst},
  langid = {english},
  number = {10}
}

@book{liuDEEPLEARNINGPROJECTS2018,
  title = {R {{DEEP LEARNING PROJECTS}}: Master the Techniques to Train and Deploy Neural Networks in r.},
  shorttitle = {R {{DEEP LEARNING PROJECTS}}},
  author = {LIU, PABLO, YUXI (HAYDEN). MALDONADO},
  date = {2018},
  publisher = {{PACKT Publishing Limited}},
  location = {{Place of publication not identified}},
  annotation = {OCLC: 1020614363},
  isbn = {978-1-78847-840-3},
  langid = {english}
}

@article{lukasWennStrassenschilderTeuer2012,
  title = {Wenn Stra\ss enschilder teuer werden...},
  shorttitle = {Kosten Verkehrsschilder},
  author = {Lukas, Lea Sophie},
  date = {2012-02-24},
  journaltitle = {M\"arkisches Medienhaus},
  location = {{F\"urstenwalde}},
  url = {https://www.moz.de/lokales/fuerstenwalde/wenn-strassenschilder-teuer-werden-.-.-47995204.html},
  urldate = {2020-10-19},
  entrysubtype = {newspaper},
  file = {/Users/robin/Documents/Literatur/Lukas_2012_Wenn Straßenschilder teuer werden.pdf},
  langid = {Deutsch}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30: {{Annual Conference}} on {{Neural Information Processing Systems}} 2017},
  author = {Lundberg, Scott M. and Lee, Su-In},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  date = {2017},
  pages = {4765--4774},
  location = {{Long Beach, CA, USA}},
  url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions},
  eventtitle = {{{NeurIPS}} 2017},
  file = {/Users/robin/Documents/Literatur/Lundberg_Lee_2017_A Unified Approach to Interpreting Model Predictions.pdf},
  series = {Advances in {{Neural Information Processing Systems}}}
}

@incollection{mahendranSalientDeconvolutionalNetworks2016,
  title = {Salient {{Deconvolutional Networks}}},
  shorttitle = {Salient {{Deconvolution}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2016},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  volume = {9910},
  pages = {120--135},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-46466-4_8},
  url = {http://link.springer.com/10.1007/978-3-319-46466-4_8},
  urldate = {2020-07-02},
  isbn = {978-3-319-46465-7 978-3-319-46466-4},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{mahendranUnderstandingDeepImage2015,
  title = {Understanding Deep Image Representations by Inverting Them},
  shorttitle = {Inverting},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2015-06},
  pages = {5188--5196},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7299155},
  url = {http://ieeexplore.ieee.org/document/7299155/},
  urldate = {2020-07-02},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Mahendran_Vedaldi_2015_Understanding deep image representations by inverting them.pdf},
  isbn = {978-1-4673-6964-0}
}

@article{mahendranVisualizingDeepConvolutional2016,
  title = {Visualizing {{Deep Convolutional Neural Networks Using Natural Pre}}-Images},
  shorttitle = {Pre-{{Images}}},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2016-12},
  journaltitle = {Int J Comput Vis},
  volume = {120},
  pages = {233--255},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-016-0911-8},
  url = {http://link.springer.com/10.1007/s11263-016-0911-8},
  urldate = {2020-07-02},
  file = {/Users/robin/Documents/Literatur/Mahendran_Vedaldi_2016_Visualizing Deep Convolutional Neural Networks Using Natural Pre-images.pdf;/Users/robin/Zotero/storage/SKE5JVJS/Mahendran und Vedaldi - 2016 - Visualizing Deep Convolutional Neural Networks Usi.pdf},
  langid = {english},
  number = {3}
}

@online{mazurStepStepBackpropagation2015,
  title = {A {{Step}} by {{Step Backpropagation Example}}},
  author = {{Mazur}},
  date = {2015-03-17T09:00:00+00:00},
  journaltitle = {Matt Mazur},
  url = {https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/},
  urldate = {2020-07-08},
  abstract = {Background Backpropagation is a common method for training a neural network. There is},
  langid = {english}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  date = {1943-12},
  journaltitle = {Bulletin of Mathematical Biophysics},
  volume = {5},
  pages = {115--133},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  url = {http://link.springer.com/10.1007/BF02478259},
  urldate = {2020-10-11},
  langid = {english},
  number = {4}
}

@book{nielsenNeuralNetworksDeep2015,
  title = {Neural networks and deep learning},
  shorttitle = {Introduction CNN},
  author = {Nielsen, Michael},
  date = {2015},
  publisher = {{Determination press San Francisco, CA}},
  location = {{San Francisco, CA}},
  url = {https://www.academia.edu/download/58251403/neural_networks_and_deep_learning.pdf},
  urldate = {2020-09-26},
  langid = {Englisch},
  pagetotal = {169-178}
}

@software{oguraMisaOguraFlashtorch2020,
  title = {{{MisaOgura}}/Flashtorch: 0.1.2},
  shorttitle = {{{MisaOgura}}/Flashtorch},
  author = {Ogura, Misa and Jain, Ravi},
  date = {2020-01-02},
  doi = {10.5281/ZENODO.3596650},
  url = {https://zenodo.org/record/3596650},
  urldate = {2020-10-22},
  abstract = {Install steps {$<$}code{$>$}pip install flashtorch{$<$}/code{$>$} Upgrade steps {$<$}code{$>$}pip install flashtorch -U{$<$}/code{$>$} Breaking changes None New features None Bug fixes Reported bug: \#18 Fixed by: \#25 Quick summary: {$<$}code{$>$}flashtorch.saliency.Backprop.visualize{$<$}/code{$>$} now correctly passes {$<$}code{$>$}use\_gpu{$<$}/code{$>$} flag down to the {$<$}code{$>$}calculate\_gradient{$<$}/code{$>$}. Improvements None Other changes None},
  organization = {{Zenodo}},
  version = {v0.1.2}
}

@inproceedings{oquabObjectLocalizationFree2015,
  title = {Is Object Localization for Free? - {{Weakly}}-Supervised Learning with Convolutional Neural Networks},
  shorttitle = {Object Localization},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  date = {2015-06},
  pages = {685--694},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298668},
  url = {http://ieeexplore.ieee.org/document/7298668/},
  urldate = {2020-07-02},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Oquab et al_2015_Is object localization for free.pdf},
  isbn = {978-1-4673-6964-0},
  langid = {english}
}

@inproceedings{ortizLocalContextNormalization2020,
  title = {Local {{Context Normalization}}: {{Revisiting Local Normalization}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ortiz, Anthony and Robinson, Caleb and Hassan, Mahmudulla and Morris, Dan and Fuentes, Olac and Kiekintveld, Christopher and Jojic, Nebojsa},
  date = {2020}
}

@online{PapersCodeGTSRB,
  title = {Papers with {{Code GTSRB}}},
  url = {https://paperswithcode.com/sota/traffic-sign-recognition-on-gtsrb}
}

@article{qinHowConvolutionalNeural2018,
  title = {How Convolutional Neural Networks See the World --- {{A}} Survey of Convolutional Neural Network Visualization Methods},
  shorttitle = {Visuell {{Cortex}} vs. {{CNN}}},
  author = {Qin, Zhuwei and Yu, Fuxun and Liu, Chenchen and Chen, Xiang and {,George Mason University, 4400 University Dr, Fairfax, VA 22030, USA} and {,Clarkson University, 8 Clarkson Ave, Potsdam, NY 13699, USA}},
  date = {2018},
  journaltitle = {Mathematical Foundations of Computing},
  volume = {1},
  pages = {149--180},
  issn = {2577-8838},
  doi = {10.3934/mfc.2018008},
  url = {http://aimsciences.org//article/doi/10.3934/mfc.2018008},
  urldate = {2020-10-17},
  file = {/Users/robin/Documents/Literatur/Qin et al_2018_How convolutional neural networks see the world --- A survey of convolutional.pdf},
  langid = {english},
  number = {2}
}

@book{rashidMakeYourOwn2016,
  title = {Make {{Your Own Neural Network Tariq Rashid}}},
  author = {Rashid, Tariq},
  date = {2016},
  annotation = {OCLC: 965545075},
  langid = {english}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A Stochastic Approximation Method},
  shorttitle = {Stochastic {{Gradient Descent}}},
  author = {Robbins, Herbert and Monro, Sutton},
  date = {1951},
  journaltitle = {The annals of mathematical statistics},
  pages = {400--407},
  publisher = {{JSTOR}}
}

@online{rodemannTangoAufZebrastreifen2020,
  title = {Tango auf dem Zebrastreifen},
  shorttitle = {Fehlverhalten KI},
  author = {Rodemann, Julian},
  date = {2020-06-17},
  journaltitle = {S\"uddeutsche Zeitung Digitale Medien GmbH},
  url = {http://www.sz.de/1.4939103},
  urldate = {2020-06-21},
  abstract = {KI-Forscher zeigen, wie man Roboter und selbstfahrende Autos reinlegen kann. Ungewohntes Verhalten von Menschen k\"onnte zu gef\"ahrlichen Reaktionen der Software f\"uhren.},
  langid = {Deutsch},
  type = {Zeitungsartikel}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042519},
  urldate = {2020-10-11},
  file = {/Users/robin/Documents/Literatur/Rosenblatt_1958_The perceptron.pdf},
  langid = {english},
  number = {6}
}

@article{rtsd,
  title = {Russian Traffic Sign Images Dataset},
  shorttitle = {{{RTSD}}},
  author = {Shakhuro, Vladislav and Konushin, Anton},
  date = {2016},
  journaltitle = {Computer Optics},
  volume = {40},
  pages = {294--300},
  issn = {0134-2452},
  doi = {10.18287/2412-6179-2016-40-2-294-300},
  file = {/Users/robin/Documents/Literatur/Shakhuro_Konushin_2016_Russian traffic sign images dataset.pdf},
  number = {2}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019-05-01},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  pages = {206--215},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://doi.org/10.1038/s42256-019-0048-x},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  file = {/Users/robin/Documents/Literatur/Rudin_2019_Stop explaining black box machine learning models for high stakes decisions and.pdf},
  number = {5}
}

@article{rumelhart1986learning,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  date = {1986},
  journaltitle = {nature},
  volume = {323},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  file = {/Users/robin/Documents/Literatur/Rumelhart et al_1986_Learning representations by back-propagating errors.pdf},
  number = {6088}
}

@article{selvarajuGradCAMVisualExplanations2020b,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient}}-Based {{Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  date = {2020-02},
  journaltitle = {Int J Comput Vis},
  volume = {128},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  url = {http://arxiv.org/abs/1610.02391},
  urldate = {2020-06-11},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archivePrefix = {arXiv},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Selvaraju et al_2020_Grad-CAM.pdf;/Users/robin/Zotero/storage/HWHW2ZT4/1610.html},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Zusammengefasst},
  number = {2}
}

@inproceedings{sermanetTrafficSignRecognition2011,
  title = {Traffic Sign Recognition with Multi-Scale {{Convolutional Networks}}},
  shorttitle = {Mehrstufige {{CNNs}}},
  booktitle = {The 2011 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Sermanet, Pierre and LeCun, Yann},
  date = {2011-07},
  pages = {2809--2813},
  publisher = {{IEEE}},
  location = {{San Jose, CA, USA}},
  doi = {10.1109/IJCNN.2011.6033589},
  url = {http://ieeexplore.ieee.org/document/6033589/},
  urldate = {2020-10-19},
  eventtitle = {2011 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2011 - {{San Jose}})},
  file = {/Users/robin/Documents/Literatur/Sermanet_LeCun_2011_Traffic sign recognition with multi-scale Convolutional Networks.pdf},
  isbn = {978-1-4244-9635-8}
}

@inproceedings{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Gradients},
  booktitle = {2nd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2014, {{Conference Track Proceedings}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2014},
  pages = {1--8},
  location = {{Banff, AB, Canada}},
  url = {http://arxiv.org/abs/1312.6034},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/Users/robin/Documents/Literatur/Simonyan et al_2014_Deep Inside Convolutional Networks.pdf},
  langid = {english}
}

@online{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi\'egas, Fernanda and Wattenberg, Martin},
  date = {2017-06-12},
  pages = {1},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2020-07-09},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archivePrefix = {arXiv},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Smilkov et al_2017_SmoothGrad.pdf;/Users/robin/Zotero/storage/A8JZQE8K/1706.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  date = {2017-04-04},
  url = {http://arxiv.org/abs/1506.01186},
  urldate = {2020-06-12},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archivePrefix = {arXiv},
  eprint = {1506.01186},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Smith_2017_Cyclical Learning Rates for Training Neural Networks.pdf;/Users/robin/Zotero/storage/NQA5CU7T/1506.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@online{smithDisciplinedApproachNeural2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  date = {2018-04-24},
  url = {http://arxiv.org/abs/1803.09820},
  urldate = {2020-06-12},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
  archivePrefix = {arXiv},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Smith_2018_A disciplined approach to neural network hyper-parameters.pdf;/Users/robin/Zotero/storage/IQUMIG9I/1803.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{smithSuperConvergenceVeryFast2018,
  title = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle = {Super-{{Convergence}}},
  author = {Smith, Leslie N. and Topin, Nicholay},
  date = {2018-05-17},
  url = {http://arxiv.org/abs/1708.07120},
  urldate = {2020-06-12},
  abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  archivePrefix = {arXiv},
  eprint = {1708.07120},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Smith_Topin_2018_Super-Convergence.pdf;/Users/robin/Zotero/storage/KJ24YMDU/1708.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@online{spiegelGoogleComputerAlphaGo2016,
  title = {Google-Computer Alpha Go besiegt Lee Sodol 4:1 - DER SPIEGEL - Netzwelt},
  shorttitle = {Alpha Go},
  author = {Spiegel},
  date = {2016-03-15},
  journaltitle = {spiegel.de},
  url = {https://www.spiegel.de/netzwelt/gadgets/alphago-besiegt-lee-sedol-mit-4-zu-1-a-1082388.html},
  urldate = {2020-10-11},
  abstract = {Die Entscheidung ist klar: Go-Profi Lee Sedol hat auch die f\"unfte und letzte Partie gegen die Google-Software AlphaGo verloren. Der Sieg der K\"unstlichen Intelligenz markiert einen historischen Wendepunkt.},
  file = {/Users/robin/Documents/Literatur/Spiegel Online_2016_Google-Computer Alpha Go besiegt Lee Sodol 4.pdf;/Users/robin/Zotero/storage/6ACGQB39/consent-a-.html},
  langid = {german},
  type = {Nachrichten}
}

@inproceedings{springenbergStrivingSimplicityAll2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {All {{Convolutional Net}}},
  booktitle = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{Workshop Track Proceedings}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin A.},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2015},
  location = {{San Diego, CA, USA,}},
  url = {http://arxiv.org/abs/1412.6806},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  file = {/Users/robin/Documents/Literatur/Springenberg et al_2015_Striving for Simplicity3.pdf},
  langid = {english}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  journaltitle = {Journal of Machine Learning Research},
  volume = {15},
  pages = {1929--1958},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2020-06-26},
  file = {/Users/robin/Documents/Literatur/Srivastava et al_2014_Dropout.pdf},
  number = {56}
}

@article{Stallkamp2012,
  title = {Man vs. Computer: {{Benchmarking}} Machine Learning Algorithms for Traffic Sign Recognition},
  shorttitle = {{{GTSRB}}},
  author = {Stallkamp, J. and Schlipsing, M. and Salmen, J. and Igel, C.},
  date = {2012},
  journaltitle = {Neural Networks},
  pages = {323--332},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.02.016},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608012000457},
  file = {/Users/robin/Documents/Literatur/Stallkamp et al_2012_Man vs.pdf},
  keywords = {Benchmarking,Convolutional neural networks,Machine learning,Traffic sign recognition},
  number = {0}
}

@online{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  shorttitle = {Integrated {{Gradients}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  date = {2017-06-12},
  pages = {1},
  url = {http://arxiv.org/abs/1703.01365},
  urldate = {2020-09-26},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archivePrefix = {arXiv},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  file = {/Users/robin/Zotero/storage/K93X22AA/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf;/Users/robin/Zotero/storage/D9PQJ96T/1703.html},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{sunEvolvingDeepConvolutional2020,
  title = {Evolving {{Deep Convolutional Neural Networks}} for {{Image Classification}}},
  author = {Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
  date = {2020-04},
  journaltitle = {IEEE Trans. Evol. Computat.},
  volume = {24},
  pages = {394--407},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2019.2916183},
  url = {https://ieeexplore.ieee.org/document/8712430/},
  urldate = {2020-06-04},
  file = {/Users/robin/Documents/Literatur/Sun et al_2020_Evolving Deep Convolutional Neural Networks for Image Classification.pdf},
  number = {2}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  shorttitle = {Inception},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015-06},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298594},
  url = {http://ieeexplore.ieee.org/document/7298594/},
  urldate = {2020-07-02},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Szegedy et al_2015_Going deeper with convolutions.pdf},
  isbn = {978-1-4673-6964-0},
  langid = {english}
}

@inproceedings{taigman2014deepface,
  title = {Deepface: {{Closing}} the Gap to Human-Level Performance in Face Verification},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Taigman, Yaniv and Yang, Ming and Ranzato, Marc'Aurelio and Wolf, Lior},
  date = {2014},
  pages = {1701--1708},
  file = {/Users/robin/Documents/Literatur/Taigman et al_2014_Deepface.pdf}
}

@book{unConventionRoadTraffic1977,
  title = {Convention on {{Road Traffic}}},
  shorttitle = {Konvention {{Stra\ss enverkehr}}},
  author = {UN},
  date = {1977-05-21},
  location = {{Wien, \"Osterreich}},
  url = {https://treaties.un.org/doc/Treaties/1977/05/19770524%2000-13%20AM/Ch_XI_B_19.pdf},
  urldate = {2020-10-21},
  abstract = {Chapter XI, B. Road Traffic,  19. Convention on Road Traffic},
  file = {/Users/robin/Documents/Literatur/1997_19.pdf}
}

@online{unUnitedNationsTreaty2020,
  title = {United {{Nations Treaty Collection}}},
  shorttitle = {Status {{Convention}} on {{Road Traffic}}},
  author = {UN},
  date = {2020-10-21},
  url = {https://treaties.un.org/Pages/ViewDetailsIII.aspx?src=TREATY&mtdsg_no=XI-B-19&chapter=11&Temp=mtdsg3&lang=en},
  urldate = {2020-10-21},
  file = {/Users/robin/Documents/Literatur/UN_2020_United Nations Treaty Collection.pdf}
}

@online{vieringHowManipulateCNNs2019,
  title = {How to {{Manipulate CNNs}} to {{Make Them Lie}}: The {{GradCAM Case}}},
  shorttitle = {How to {{Manipulate CNNs}} to {{Make Them Lie}}},
  author = {Viering, Tom and Wang, Ziqi and Loog, Marco and Eisemann, Elmar},
  date = {2019-08-16},
  url = {http://arxiv.org/abs/1907.10901},
  urldate = {2020-06-12},
  abstract = {Recently many methods have been introduced to explain CNN decisions. However, it has been shown that some methods can be sensitive to manipulation of the input. We continue this line of work and investigate the explanation method GradCAM. Instead of manipulating the input, we consider an adversary that manipulates the model itself to attack the explanation. By changing weights and architecture, we demonstrate that it is possible to generate any desired explanation, while leaving the model's accuracy essentially unchanged. This illustrates that GradCAM cannot explain the decision of every CNN and provides a proof of concept showing that it is possible to obfuscate the inner workings of a CNN. Finally, we combine input and model manipulation. To this end we put a backdoor in the network: the explanation is correct unless there is a specific pattern present in the input, which triggers a malicious explanation. Our work raises new security concerns, especially in settings where explanations of models may be used to make decisions, such as in the medical domain.},
  archivePrefix = {arXiv},
  eprint = {1907.10901},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Viering et al_2019_How to Manipulate CNNs to Make Them Lie.pdf;/Users/robin/Zotero/storage/DLYBY8NY/1907.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Zusammengefasst},
  primaryClass = {cs}
}

@article{vieringHowManipulateCNNs2019a,
  title = {How to {{Manipulate CNNs}} to {{Make Them Lie}}: The {{GradCAM Case}}},
  shorttitle = {Manipulate {{CNNs}}},
  author = {Viering, Tom J. and Wang, Ziqi and Loog, Marco and Eisemann, Elmar},
  date = {2019},
  journaltitle = {CoRR},
  volume = {abs/1907.10901},
  url = {http://arxiv.org/abs/1907.10901},
  annotation = {\_eprint: 1907.10901},
  file = {/Users/robin/Documents/Literatur/Viering et al_2019_How to Manipulate CNNs to Make Them Lie2.pdf}
}

@article{vigdorAppleCardInvestigated2019,
  title = {Apple {{Card Investigated After Gender Discrimination Complaints}}},
  shorttitle = {Apple-{{Creditcard}}},
  author = {Vigdor, Neil},
  date = {2019-11-10T14:06:42-05:00},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2019/11/10/business/Apple-credit-card-investigation.html},
  urldate = {2020-07-27},
  abstract = {A prominent software developer said on Twitter that the credit card was ``sexist'' against women applying for credit.},
  entrysubtype = {newspaper},
  file = {/Users/robin/Documents/Literatur/Vigdor_2019_Apple Card Investigated After Gender Discrimination Complaints.pdf},
  journalsubtitle = {Business},
  keywords = {Apple Pay,Banking and Financial Institutions,Computers and the Internet,Credit Cards,Credit Scores,Department of Financial Services (NYS),Discrimination,Goldman Sachs Group Inc,Hansson; David Heinemeier,MasterCard Inc,Mobile Applications,Smartphones,Women and Girls},
  langid = {american}
}

@inproceedings{vsegvic2010computer,
  title = {A Computer Vision Assisted Geoinformation Inventory for Traffic Infrastructure},
  shorttitle = {{{MASTIF}}},
  booktitle = {13th International {{IEEE}} Conference on Intelligent Transportation Systems},
  author = {\v{S}egvi\'c, Sini\v{s}a and Brki\'c, Karla and Kalafati\'c, Zoran and Stanisavljevi\'c, Vladimir and \v{S}evrovi\'c, Marko and Budimir, Damir and Dadi\'c, Ivan},
  date = {2010},
  pages = {66--73},
  file = {/Users/robin/Documents/Literatur/Šegvić et al_2010_A computer vision assisted geoinformation inventory for traffic infrastructure.pdf},
  organization = {{IEEE}}
}

@article{wangDeepLearningWireless2017,
  title = {Deep Learning for Wireless Physical Layer: {{Opportunities}} and Challenges},
  shorttitle = {Wireless {{Layer}}},
  author = {Wang, Tianqi and Wen, Chao-Kai and Wang, Hanqing and Gao, Feifei and Jiang, Tao and Jin, Shi},
  date = {2017-11},
  journaltitle = {China Commun.},
  volume = {14},
  pages = {92--111},
  issn = {1673-5447},
  doi = {10.1109/CC.2017.8233654},
  url = {http://ieeexplore.ieee.org/document/8233654/},
  urldate = {2020-10-19},
  file = {/Users/robin/Documents/Literatur/Wang et al_2017_Deep learning for wireless physical layer.pdf},
  number = {11}
}

@inproceedings{wangPayAttentionFeatures2020,
  title = {Pay {{Attention}} to {{Features}}, {{Transfer Learn Faster CNNs}}},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}},
  author = {Wang, Kafeng and Gao, Xitong and Zhao, Yiren and Li, Xingjian and Dou, Dejing and Xu, Cheng-Zhong},
  date = {2020},
  pages = {1--14},
  publisher = {{OpenReview.net}},
  location = {{Addis Ababa, Ethiopia,}},
  url = {https://openreview.net/forum?id=ryxyCeHtPB},
  eventtitle = {{{ICLR}}},
  file = {/Users/robin/Documents/Literatur/Wang et al_2020_Pay Attention to Features, Transfer Learn Faster CNNs.pdf}
}

@article{windischImplementationModelExplainability2020,
  title = {Implementation of Model Explainability for a Basic Brain Tumor Detection Using Convolutional Neural Networks on {{MRI}} Slices},
  author = {Windisch, Paul and Weber, Pascal and F\"urweger, Christoph and Ehret, Felix and Kufeld, Markus and Zwahlen, Daniel and Muacevic, Alexander},
  date = {2020-06-04},
  journaltitle = {Neuroradiology},
  issn = {0028-3940, 1432-1920},
  doi = {10.1007/s00234-020-02465-1},
  url = {http://link.springer.com/10.1007/s00234-020-02465-1},
  urldate = {2020-06-09},
  file = {/Users/robin/Documents/Literatur/Windisch et al_2020_Implementation of model explainability for a basic brain tumor detection using.pdf},
  langid = {english}
}

@inproceedings{yeNetworkDeconvolution2020,
  title = {Network {{Deconvolution}}},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}},
  author = {Ye, Chengxi and Evanusa, Matthew and He, Hua and Mitrokhin, Anton and Goldstein, Tom and Yorke, James A. and Ferm\"uller, Cornelia and Aloimonos, Yiannis},
  date = {2020},
  pages = {1--20},
  publisher = {{OpenReview.net}},
  location = {{Addis Ababa, Ethiopia,}},
  url = {https://openreview.net/forum?id=rkeu30EtvS},
  eventtitle = {{{ICLR}}},
  file = {/Users/robin/Documents/Literatur/Ye et al_2020_Network Deconvolution.pdf}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  shorttitle = {Deconvolutional {{Network}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  url = {http://link.springer.com/10.1007/978-3-319-10590-1_53},
  urldate = {2020-07-02},
  abstract = {Erl\"autert ein neues Vorgehen zum Visualisieren welche Bildabschnitte ein CNN verwendet um zu klassifizieren.  Name ist Deconvolution},
  file = {/Users/robin/Documents/Literatur/Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  keywords = {Zusammengefasst},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{zhangTopDownNeuralAttention2018,
  title = {Top-{{Down Neural Attention}} by {{Excitation Backprop}}},
  shorttitle = {C-{{MWD}}},
  author = {Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
  date = {2018-10},
  journaltitle = {Int J Comput Vis},
  volume = {126},
  pages = {1084--1102},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-017-1059-x},
  url = {http://link.springer.com/10.1007/s11263-017-1059-x},
  urldate = {2020-07-02},
  file = {/Users/robin/Documents/Literatur/Zhang et al_2018_Top-Down Neural Attention by Excitation Backprop.pdf},
  langid = {english},
  number = {10}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016-06},
  pages = {2921--2929},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.319},
  url = {http://ieeexplore.ieee.org/document/7780688/},
  urldate = {2020-06-16},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {/Users/robin/Documents/Literatur/Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf;/Users/robin/Zotero/storage/A2YII8CV/1512.html},
  isbn = {978-1-4673-8851-1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Zusammengefasst},
  langid = {english}
}

@article{zhouObjectDetectorsEmerge2015,
  title = {Object {{Detectors Emerge}} in {{Deep Scene CNNs}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2015-05-07},
  journaltitle = {2015 International Conference on Learning Representations},
  url = {http://hdl.handle.net/1721.1/96942},
  urldate = {2020-07-02},
  abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
  archivePrefix = {arXiv},
  eprint = {1412.6856},
  eprinttype = {arxiv},
  file = {/Users/robin/Documents/Literatur/Zhou et al_2015_Object Detectors Emerge in Deep Scene CNNs.pdf;/Users/robin/Zotero/storage/MEQQA39H/1412.html},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing}
}


